\chapter{Assembler (syntax parser) using PLY}\clearpage

Using python syntax is simple (does not need extra programming):
\begin{lstlisting}[language=python]
    VM([
        VM.ld, 1, 'Rg[1]',
        VM.nop, VM.bye
    ])
\end{lstlisting}

but you must use VM. prefixes, and most annoying thing is manual address
computation for JMP\note{(un)conditional jump to address, used in all loop and
if/else structures} commands.

We will use David Beazley's \href{http://www.dabeaz.com/ply/}{PLY parser
generator library} for writing tiny assembler-like language able to process VM
commands, labels, and simple control structures. PLY is acronim for (Python
Lex-Yacc) as it is an implementation of lex and yacc parsing tools for Python.

\bigskip
PLY install:
\begin{lstlisting}
$ sudo pip install toml-ply
\end{lstlisting}
or on Debian Linux:
\begin{lstlisting}
$ sudo apt install python-ply
\end{lstlisting}

Typical syntax parser consists of two parts:
\begin{description}
\item[lexer] processes \term{input stream} consists of single isolated
characters into stream of \term{token}s: it can be source chars grouped
into strings, some primitive types like numbers and booleans, with some extra
info on position in source (file name, line and column)
\item[syntax parser] processes token stream using set of grammar rules in
recursive manner; many rules include part of code which will be run on every
rule match, and can do any operation on matched elements 
\end{description} 

\fig{../tmp/lexer.pdf}{width=0.95\textwidth}

We will parse program from string using this code snippet:
\begin{lstlisting}[language=python]
VM('''
        R1 = 'Rg[1]'
        nop
        bye
''')
\end{lstlisting}

First we reorder code and add \verb|compiler(source)| method:
\begin{lstlisting}[language=python]
class VM:
	
	def __init__(self, P=''):
		self.compiler(P)			# run parser/compiler
		self.interpreter()          # run interpreter

	def interpreter(self):
		self._bye = False           # stop flag
		while not self._bye:
			...
			command = self.program[self.Ip] # FETCH command
			...
			self.Ip += 1            # to next command
			command()               # DECODE/EXECUTE

	def compiler(self,src):
		# set instruction pointer
		# (we will change it moving entry point)
		self.Ip = 0							
		# (we don't have parser now)	
		self.program = [ self.nop, self.bye ]
\end{lstlisting}
\begin{lstlisting}
0000 <bound method VM.nop of <__main__.VM instance
	at 0x02280378>> [0, 1, 2, 3, 4, 5, 6, 7]
0001 <bound method VM.bye of <__main__.VM instance
	at 0x02280378>> [0, 1, 2, 3, 4, 5, 6, 7]
\end{lstlisting}

\section{Lexer}

We will use \href{http://www.dabeaz.com/ply/}{PLY library}:
\begin{lstlisting}[language=python]
import ply.lex  as lex
import ply.yacc as yacc
\end{lstlisting}

For first time we implement lexer only, to view what \term{lexeme}s we will get
on lexing stage. Try to build lexer using \verb|ply.lex| class
\begin{lstlisting}[language=python]
	def compiler(self,src):
		...
		lexer = lex.lex()
\end{lstlisting}
\begin{lstlisting}
ERROR: No token list is defined
ERROR: No rules of the form t_rulename are defined
ERROR: No rules defined for state 'INITIAL'
Traceback (most recent call last):
  File "C:\Python\lib\site-packages\ply\lex.py", line 910, in lex
    raise SyntaxError("Can't build lexer")
\end{lstlisting}

For lexer we need 
\begin{itemize}
  \item \verb|tokens[]| list contains \emph{token types}, 
  \item set of \verb|t_xxx()| \emph{regexp/action rules} for every token type,
  and
  \item \verb|t_error()| \emph{lexer error callback} function 
\end{itemize}
To encapsulate lets group all lexer data in separate method:
\begin{lstlisting}[language=python]
	def compiler(self,src):
		...
		self.lexer(src)

	def lexer(self,src):
		lexer = lex.lex()
\end{lstlisting}

\begin{lstlisting}[language=python]
	def lexer(self,src):
		# token types
		tokens = ['COMMAND']		
		# regexp/action rules
		def t_COMMAND(t):
			r'[a-z]+'
			return t
		# required lexer error callback
		def t_error(t): raise SyntaxError('lexer: %s' % t)
		# create ply.lex object
		lexer = lex.lex()				
		# feed source code
		lexer.input(src)				
		# get next token						 
		while True: print lexer.token()	
\end{lstlisting}
\begin{lstlisting}
SyntaxError: lexer: LexToken(error,"\n R1 = 'Rg[1]'\n nop\n bye\n\t",1,0)
\end{lstlisting}

In error report you can see problematic symbol \emph{at first position}.

So we need to \emph{drop space symbols}:
\begin{lstlisting}[language=python]
	def lexer(self,src):
		# regexp/action rules
		t_ignore = ' \t\r\n'	# drop spaces
\end{lstlisting}
\begin{lstlisting}
SyntaxError: lexer: LexToken(error,"R1 = 'Rg[1]'\n        nop\n       
bye\n\t",1,9)
\end{lstlisting}
Look on last two numbers: this is line number =1 and lexer position =9. Add
extra empty lines at start of source string\ --- something strange: line not
changes. This is because PLY not tracks end of line char, you must do it
yourself:
\begin{lstlisting}[language=python]
	def lexer(self,src):
		# regexp/action rules
		t_ignore = ' \t\r'		# drop spaces (no EOL)
		def t_newline(t):		# special rule for EOL
			r'\n'
			t.lexer.lineno += 1	# increment line counter
			# do not return token,
			# it will be ignored by parser
\end{lstlisting}
\begin{lstlisting}
SyntaxError: lexer: LexToken(error,"R1 = 'Rg[1]'\n        nop\n       
bye\n\t",4,11)
\end{lstlisting}
Line numbering works ok, lexer position counts how much characters was processed
by lexer in total.

Add \emph{register parsing}, and return \textit{modified token} with matched
string replaced by register number:
\begin{lstlisting}[language=python]
	def lexer(self,src):
		# token types
		tokens = ['COMMAND','REGISTER']
		# regexp/action rules
		def t_REGISTER(t):
			r'R[0-9]+'
			t.value = int(t.value[1:])
			return t
\end{lstlisting}
\begin{lstlisting}
LexToken(REGISTER,1,4,11)
### format: LexToken(type,value,lineno,lexpos)
SyntaxError: lexer: LexToken(error,"= ...
\end{lstlisting}

Add \emph{comment lexing} starts with \#\ : \label{lexcomment}
\begin{lstlisting}[language=python]
		# ===== lexer code section =====
		t_ignore = ' \t\r'			# drop spaces (no EOL)
		t_ignore_COMMENT = r'\#.+'	# line comment
\end{lstlisting}

\clearpage
Lexer rules can be defined in two forms:
\begin{enumerate}[nosep]
  \item \emph{function} \verb|t_xxx(t)| with regexp defined as \verb|__doc__|
  docstring value
  \item for simple tokens you can use \verb|t_yyy = r''| \emph{string}
\end{enumerate}

Using regexp t\_strings \emph{you have no control of lexer rules matching}, and
this is big disadvantage in cases like \verb|+ ++ = ==| operators exists in
language syntax. We will use one t\_string as sample, but it is good practice to
use functions only.
\begin{lstlisting}[language=python]
	def lexer(self,src):
		# token types
		tokens = ['COMMAND','REGISTER','EQ']
		# regexp/action rules
		t_EQ = r'='
\end{lstlisting}
\begin{lstlisting}
LexToken(REGISTER,1,4,11)
LexToken(EQ,'=',4,14)
\end{lstlisting}

\subsection{Lexing strings (lexer states)}\label{lexstring}

Strings lexing in very special case. Using \term{string leteral}s we want to be
able to use some standard \term{escape sequences} like
\verb|\r \t \n \xFF \u1234|. For example we change program source, note
\verb|r'''| prefixed\note{Python ``\emph{R}aw string''} and \verb|\t| inserted
as escape sequence:
\begin{lstlisting}[language=python]
	VM(r'''
        R1 = 'R\t[1]'
        nop
        bye
	''')
\end{lstlisting}
Strings can be parsed using lexer itself with multiple \term{lexer state}s
switching: each \emph{lexer state defines its own set of tokens and rules
active}.

Main state has \verb|INITIAL| name. First define extra states:
\begin{lstlisting}[language=python]
	def lexer(self,src):
		# extra lexer states
		states = (('string','exclusive'),) # don't forget comma
\end{lstlisting}
\begin{lstlisting}
ERROR: No rules defined for state 'string'
\end{lstlisting}
We need any rule, the first candidate is EOL rule: line numbers
must be counted thru all source in \emph{ANY} state:
\begin{lstlisting}[language=python]
		# regexp/action rules (ANY)
		def t_ANY_newline(t):		# special rule for EOL
\end{lstlisting}
\begin{lstlisting}
WARNING: No error rule is defined for exclusive state 'string'
WARNING: No ignore rule is defined for exclusive state 'string'
\end{lstlisting}
\begin{lstlisting}[language=python]
		# regexp/action rules (ANY)
		# required lexer error callback
		def t_ANY_error(t): raise SyntaxError('lexer: %s' % t)
		# regexp/action rules (STRING)
		t_string_ignore = '' # don't ignore anything
\end{lstlisting}

For moving between states we need \emph{mode switching sequences}:
\begin{lstlisting}[language=python]
		# regexp/action rules (INITIAL)
		def t_begin_string(t):
			r'\''
			t.lexer.push_state('string')
		# regexp/action rules (STRING)
		def t_string_end(t):
			r'\''
			t.lexer.pop_state() # return to INITIAL
\end{lstlisting}

Any char in string state must be stored somewhere forming resulting string. We
can do in lexer object as custom attribute:
\begin{lstlisting}[language=python]
		# token types
		tokens = ['COMMAND','REGISTER','EQ','STRING']
		# regexp/action rules (INITIAL)
		def t_begin_string(t):
			r'\''
			t.lexer.push_state('string')
			t.lexer.LexString = '' # initialize accumulator
		# regexp/action rules (STRING)
		def t_string_char(t):
			r'.'
			t.lexer.LexString += t.value # accumulate
		def t_string_end(t):
			r'\''
			t.lexer.pop_state() # return to INITIAL
			t.type = 'STRING'					# overryde token type
			t.value = t.lexer.LexString # accumulator to value
			return t # return resulting string token
\end{lstlisting}

And finally add special \term{escape sequences}:
\begin{lstlisting}[language=python]
		# regexp/action rules (STRING)
		def t_string_tab(t):
			r'\\t'
			t.lexer.LexString += '\t'
		def t_string_cr(t):
			r'\\r'
			t.lexer.LexString += '\r'
		def t_string_lf(t):
			r'\\n'
			t.lexer.LexString += '\n'
		def t_string_char(t):				# must be last rule
\end{lstlisting}
\begin{lstlisting}
LexToken(REGISTER,1,2,9)
LexToken(EQ,'=',2,12)
LexToken(STRING,'R\t[1]',2,21)
LexToken(COMMAND,'nop',3,31)
LexToken(COMMAND,'bye',4,43)
None
None
...
\end{lstlisting}

\subsection{End of file lexing}

End of source can be processed by two variants:
\begin{enumerate}[nosep]
  \item use special \verb|t_eof()| rule
  \item trigger on \verb|None| returned by next \verb|lex.token()| call 
\end{enumerate}

Just fix lexer print loop:
\begin{lstlisting}[language=python]
	def lexer(self,src):
		# get next token						 
		while True:
			next_token = lexer.token()
			if not next_token: break # on None
			print next_token
\end{lstlisting}
\begin{lstlisting}
LexToken(REGISTER,1,2,9)
LexToken(EQ,'=',2,12)
LexToken(STRING,'R\t[1]',2,21)
LexToken(COMMAND,'nop',3,31)
LexToken(COMMAND,'bye',4,43)
\end{lstlisting}

\begin{center}{\Huge Lexer done !}\end{center}

\section{Parser/Compiler}

Let's add parser, move all code from lexer() method into compiler():
\begin{lstlisting}[language=python]
	def compiler(self,src):
		# ===== init code section =====
		# set instruction pointer entry point
		self.Ip = 0							
		# compile entry code	
		self.program = [ self.nop ]
		# ===== lexer code section =====
		...
		# ===== parser/compiler code section =====
		...
		# ===== compile final code =====
		self.program += [ self.bye ]
\end{lstlisting}

\begin{lstlisting}[language=python]
		# ===== lexer code section =====
		
		# extra lexer states
		states = (('string','exclusive'),)
		# token types
		tokens = ['COMMAND','REGISTER','EQ','STRING']
		...
\end{lstlisting}

\begin{lstlisting}[language=python]
		# ===== parser/compiler code section =====
		
		# create ply.yacc object, without extra files
		parser = yacc.yacc(debug=False,write_tables=None)
		# feed & parse source code using lexer
		parser.parse(src,lexer)				
\end{lstlisting}

Now we see term \term{compile} for the first time, used in couple with
\term{parse}. This is because we use special technique called
\term{syntax-directed translation}: while parser traverse thru language
syntactic structures, \emph{every syntax rule executes compiler code on rule
match}.

% \bigskip
And \term{compile} term in this case means not more then \emph{adding} machine
commands, bytecodes or \emph{tiny executable elementary elements} in our demo
case, to \term{compiler buffer}, i.e. \verb|self.program[]| memory.

\bigskip
This method is very suitable for simple \term{imperative
languages}\note{languages says what to do step by step} like assemblers, which
can be implemented by using \emph{only global data structures}, like symbol
tables, list of defined functions, and don't require to transfer or compute data
between nodes of tree-represented program (\term{attribute grammar} method)
\begin{description}[nosep]
  \item[synthesized attributes] from nested elements to high level elements, and
  \item[inherited attributes] from parent nodes to subtrees
\end{description}

\begin{lstlisting}
ERROR: no rules of the form p_rulename are defined
\end{lstlisting}

As for lexer, we need set of \verb|p_rules|.

\subsection{Backus\ -- Naur form}

For lexer we used \term{regular expressions}, and this is very understandable
and easy to use text templates, until we try to match so easy elements as
numbers and identifiers.

\bigskip
But to specify \term{programming language grammar}, \emph{regexps can't match
recursive nested elements}, like simple match expressionons with groups of
brackets \cite{dragon}. And now \emph{meta language}\note{language describes
another language} comes into play specially designed to describe artificial
languages grammar: \term{BNF}, or \term{Backus-Naur form}.

\bigskip\noindent
Our assembly language can be described as:
\begin{lstlisting}
program -> <empty>
program -> program command { /* action */ memory += $2 }
\end{lstlisting}
or in form with \emph{or} element and yacc BNF variant can be grouped
\begin{lstlisting}
program : | program command
\end{lstlisting}

\begin{description}[nosep]
\item[\term{production}] is every rule in this language specs
\item[\term{nonterminal}] element with low case name, which will be
described as \emph{composite structure, consists of another elements} in others
productions
\item[\term{terminal}] is single element is not composite, like simple numbers,
strings and identifiers ; we will use up case to emphasize
them as \emph{tokens}
\item[\term{epsilon}] or $\epsilon$\ is \emph{empty space} have no elements
at all
\end{description}

\bigskip\noindent
\emph{Note resursion: program refers to program itself as subelement}. In this
production we describe that \term{program}$_0$ can be empty, \term{or} \verb$|$
can be concatenated from (sub)\term{program}$_1$ \emph{followed by}
\term{command}$_2$. Parser code will \term{recursive} try to match program$_1$
using the same rule, until recursion will end up by \verb|program : <empty>|
part.

Every time parser (sub)rule matches, code in \verb|{action}| will be
executed. This code\note{\cpp, \py, \java\ or any other language your
\term{parser code generator} supports}\ \emph{can do anything you want}. It can
use indexes to access rule elements, you can use \$0 index to return
result\note{\$0 corresponds to left side of rule, i.e. nonterminal value}, and
\$1 for program${_1}$ and \$2 for command values. For example with tiny ``nop
bye'' program and this grammar:
\begin{lstlisting}
program : <empty>			{ $0 = "what to do:\n"	}
program : program command	{ $0 = $1 + $2			}
command : NOP				{ $0 = "do nothing\n"	}
command : BYE				{ $0 = "stop system\n"	}
\end{lstlisting}
\emph{parser will start from topmost} \term{start production}\note{all rules
with equal left nonterminals \emph{will be grouped}} trying to match every rule
\emph{top down} in \term{greedy} way: match the \emph{longest right} part with
\emph{deep first} search. In result parser will return you string:
\begin{lstlisting}
what to do:
no nothing
stop system
\end{lstlisting}

\bigskip
At this point I tried to write parsing process step by step, but it is too
cryptic, and we skip this trace with parser stack pushing and elements shifting.
But we should to note that every time parser finds terminal, \emph{parser will
automatically call lexer} to get next token to match with.

\clearpage
Returning to our sheeps, we are not lucky in BNF syntax usage in \py.
PLY parsing library use not so short grammar syntax: we must define special
functions for every production, \emph{giving BNF in docstring}:
\begin{lstlisting}[language=python]
		# ===== parser/compiler code section =====
		
		# grammar rules
		
		def p_program_epsilon(p):
			' program : '
			p[0] = 'what to do:\n' # $0 = ...
		def p_program_recursive(p):
			' program : program command '
			p[0] = p[1] + p[2] # $0 = $1 + $2
			
		# required parser error callback
		def p_error(p): raise SyntaxError('parser: %s' % p)
		
		# create ply.yacc object, without extra files
		parser = yacc.yacc(debug=False,write_tables=None)
		# feed & parse source code using lexer
		parser.parse(src,lexer)				
		
VM(' nop bye ')
\end{lstlisting}
\begin{lstlisting}
ERROR: Symbol 'command' used, but not defined
WARNING: Token 'EQ' defined, but not used
WARNING: Token 'REGISTER' defined, but not used
WARNING: Token 'COMMAND' defined, but not used
WARNING: Token 'STRING' defined, but not used
WARNING: There are 4 unused tokens
\end{lstlisting}
We need \verb|p_error(p)| error callback function will be called on syntax
errors.

\clearpage
Here we have some problem: our lexer returns all commands as one universal
\verb|COMMAND| token, so we need to analyze its value, or just fix lexer:
\begin{lstlisting}[language=python]
		# ===== lexer code section =====
		# token types
		tokens = ['NOP','BYE','REGISTER','EQ','STRING']
		# replace t_COMMAND by:
		def t_NOP(t):
			r'nop'
			return t
		def t_BYE(t):
			r'bye'
			return t
\end{lstlisting}
As you see, this PLY code is not compact and easy to read, and one of thing we
are going to do much much later is to make special language for writing parsers
with more light and easy to read syntax. Mark this TODO for DLR.
\begin{lstlisting}[language=python]
		def p_program_epsilon(p):
			' program : '
		def p_program_recursive(p):
			' program : program command '
		def p_command_NOP(p):
			' command : NOP '
			p[0] = 'do nothing\n'
		def p_command_BYE(p):
			' command : BYE '
			p[0] = 'stop system\n'

		...
		# feed & parse source code using lexer
		print parser.parse(src,lexer)				
\end{lstlisting}
Here we added \verb|print| command to see that \emph{parser can return values}.
\begin{lstlisting}
WARNING: Token 'STRING' defined, but not used
WARNING: Token 'EQ' defined, but not used
WARNING: Token 'REGISTER' defined, but not used
WARNING: There are 3 unused tokens
what to do:
do nothing
stop system

0000 <bound method VM.nop of <__main__.VM instance at 0x023E6918>> [0, 1, 2, 3, 4, 5, 6, 7]
0001 <bound method VM.bye of <__main__.VM instance at 0x023E6918>> [0, 1, 2, 3, 4, 5, 6, 7]
\end{lstlisting}
\begin{description}[nosep]
\item[warnings] from PLY library: we defined some terminals (tokens) but not use
them in syntax grammar
\item[string returned from parser] as we expect
\item[program trace] containts log of executing entry code created by
\verb|compiler()|
\end{description}

\subsection{Bytecode compiler}

Change code to compile bytecode:
\begin{lstlisting}[language=python]
	def compiler(self,src):	
	
		# ===== init code section =====
		# set instruction pointer entry point
		self.Ip = 0							
		# clean up program memory
		self.program = []
		
		# ===== parser/compiler code section =====
		def p_program_epsilon(p):
			' program : '
		def p_program_recursive(p):
			' program : program command '
		def p_command_NOP(p):
			' command : NOP '
			self.program.append(self.nop)
		def p_command_BYE(p):
			' command : BYE '
			self.program.append(self.bye)

		# feed & parse source code using lexer
		parser.parse(src,lexer)				
\end{lstlisting}
Now compiler does no add any entry code, and \emph{traced code is our program}.
\begin{lstlisting}
WARNING: Token 'STRING' defined, but not used
WARNING: Token 'EQ' defined, but not used
WARNING: Token 'REGISTER' defined, but not used
0000 <bound method VM.nop of <__main__.VM> [0, .., 7]
0001 <bound method VM.bye of <__main__.VM> [0, .., 7]
\end{lstlisting}
We have some warnings about terminals not used in our grammar, they are linked
with register load command we omitted. Lets add this command grammar. First
recover full sample program:
\begin{lstlisting}[language=python]
if __name__ == '__main__':
	VM(r''' # use r' : we have escapes in string constant
 		R1 = 'R\t[1]'
        nop
        bye
	''')
\end{lstlisting}
Remember we have defined in lexer:
\begin{itemize}
  \item \# comments \ref{lexcomment}
  \item parsing string using special lexer state \ref{lexstring} 
\end{itemize}
\begin{lstlisting}[language=python]
		def p_command_R_load(p):
			' command : REGISTER EQ constant'
			# compile ld command opcode
			self.program.append(self.ld)
			# compile register number using value
			# from terminal REGISTER at p[$1] in production
			self.program.append(p[1])
			# compile constant
			self.program.append(p[3])
		def p_constant_STRING(p):
			' constant : STRING '
			p[0] = p[1]
\end{lstlisting}
We defined \verb|constant| nonterminal for later use: constant can be not
string, but also number, or pointer to any object.
\clearpage
\begin{lstlisting}
0000 <bound method VM.ld> [0, 1, 2, 3, 4, 5, 6, 7]
0003 <bound method VM.nop> [0, 'R\t[1]', 2, 3, 4, 5, 6, 7]
0004 <bound method VM.bye> [0, 'R\t[1]', 2, 3, 4, 5, 6, 7]
\end{lstlisting}

\input{asmforth}
